---
title: "Summer_2023_RSF_Official"
output: html_document
date: "2023-09-20"
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
#### Libraries #### 
library(raster)
library(amt)
library(dplyr)
library(tibble)
library(purrr)
library(tidyr)
library(lme4)
library(lmerTest)
library(janitor)
library(rio)
library(stringr)
library(MuMIn)
library(tidyverse)
library(bayesplot)
library(ggeffects)
library(ggplot2)
library(readxl)
library(brms)
library(magrittr)
library(dplyr)
library(purrr)
library(forcats)
library(tidyr)
library(modelr)
library(ggdist)
library(tidybayes)
library(ggplot2)
library(cowplot)
library(rstan)
library(ggrepel)
library(RColorBrewer)
library(gganimate)
library(posterior)
library(distributional)
library(posterior)
library(adehabitatHR)


options(mc.cores = 4)

```

Importing map of study site, setting template for raster stack.
```{r}
##### Importing map of study site as a shape file #### 

habitat <- raster::shapefile("/Volumes/Samsung_T5/BOBWHITE_DATA/Maps/2023_LCov_BurnStatus/2023_LCOV_BurnStatus_V2.shp")

#Let's count the land cover types.
habitat %>%
  as.data.frame() %>%
  count(LCOV_MGMT2, sort = T) %>%
  print()


#Checking the extent of the bounding box.
extent(habitat@bbox)

#Creating a column that treats land cover (LCOV_MGMT2) types as a numeric factor.
habitat$hab.factor <- as.numeric(as.factor(habitat$LCOV_MGMT2))

#Viewing levels (landc over types aka HABITAT)
levels(as.factor(habitat$LCOV_MGMT2))

#Quick look
#head(habitat)

#Creating template raster. Here, we specify crs, extent, and resolution of our raster. For some reason, model won't run with a resolution below 3 meters. This shouldn't be an issues w telemetry data, as we want a larger raster cell size.
template.raster <- raster(crs = habitat@proj4string,
       ext = extent(habitat@bbox),
       res = 10)


#Rasterizing the map of study site, filling with land cover type (as numeric factors). Pretty sure this is unnecessary, as it is handled in our for() loop later.
habitat.raster <- rasterize(x = habitat,
                            y = template.raster,
                            field = habitat$hab.factor)

#plotting habitat raster habitat raster
plot(habitat.raster)
#windows()

#Viewing habitat shapefile. Land cover types here are now numeric values.
plot(habitat, col = as.factor(habitat$LCOV_MGMT2))
```

Creating our stack of raster layers. This will be used in Summer 2023 RSF.
```{r}
#### Creating raster stack #### 

#Assigning numeric land cover types to "habitats" as levels. "habitat" is our shapefile, while habitat.raster is the rasterized version of our shapefile. Unsure if it matters that we assign levels from the shapefile version.
habitats <- levels(as.factor(habitat$LCOV_MGMT2))

#Creating a raster list. This will be used in our for loop.
rasterList <- list()

#Creating our for loop. This creates a distance-based raster layer for each of our seven lcov types.
#Each raster cell in a given lcov layer will hold the distance to the nearest respective lcov type. This value will be 0 if the cell is within the lcov type. 
for(i in 1:length(habitats)){   #We'll loop through each lcov type in "habitats".
  # i <- 1 (debugger, leave muted unless needed)
  #for each layer in the stack, we create a subset called "hab" that only contains that respective lcov type.
  hab <- subset(habitat, LCOV_MGMT2 == habitats[i])   
  #assigning a value of 1 to each location in hab in a new column called field. So, in ag layer, each cell in ag$field = 1.
  hab$field <- 1 
  # plot(hab) Leave muted. (unnecessary unless you want to plot each layer as the loop runs) 
  #We rasterize hab subset in each stack, based on raster layer. We transfer values from hab$field. Lcov types outside of the respective layer (background) are assigned NA. We calc distance from each cell to nearest respective lcov type, creating a dist-based raster layer.
  rasterList[[i]] <- distance(rasterize(hab, template.raster, field = hab$field, background = NA))
  #Creating list of dist-based raster layers, naming each element after corresponding lcov type.
  names(rasterList)[i] <- habitats[i]
}

#This extracts the extent of each raster layer in our list.
lapply(rasterList, extent)

#Now we stack our raster layers.
distanceStack23 <- stack(rasterList)

#Plotting our seven distance-based raster layers.
plot(distanceStack23)
```

Importing Summer 2023 data. 
```{r}
#### Importing Summer 2023 locations #### 

#/Volumes/Samsung_T5/R_projects/_THESIS/Chapter1/S23locsUnweighted.xlsx

Summer_2023_locs <- rio::import("/Volumes/Samsung_T5/R_projects/_THESIS/Chapter1/S23locsUnweighted.xlsx", setclass = "tibble") %>%
  clean_names() %>%
  filter(status == "A") %>%
  mutate(date = lubridate::as_date(date)) %>%
  filter(date >= "2023-04-18",
         date <= "2023-09-30")

#Importing "fate" spreadsheet, which has sex associated with each band number (unlike daily locations data)
fate <- rio::import("/Volumes/Samsung_T5/BOBWHITE_DATA/Clean/Fate/fate_all_birds_clean4880EDIT.xlsx", setclass = "tibble") %>%
  clean_names()

#Adding sex to each real/random location.
Summer_2023_loc <- Summer_2023_locs %>%
  left_join(fate %>% select(band_numb, sex), by = "band_numb")

```

Creating tibbles for RSF, adding sex of nobo, and year of obs
```{r}
#### Creating Summer 2023 tibbles for RSFs ####

#Creating tibble for RSF, includes random and known points for each NOBO within kde home range, as well as extracted covariates.  
rsfData_summer2023 <- Summer_2023_locs %>% 
  #Converting it to a tibble
  as_tibble() %>%
   #Removing timezone offset, letter "T", creating DT.GMT column by converting to POSIX format, creating DT column by subtracting 5 hrs (time zones)
  mutate(DT.chr = gsub("-05:00","",gsub("T"," ",date_created)),
         DT.GMT = as.POSIXct(DT.chr, format = "%Y-%m-%d %H:%M:%S", tz = "GMT"),
         DT = DT.GMT-lubridate::hours(5)) %>% 
  #Nesting by band number into new column called indData.
  nest(indData = !band_numb) %>% 
  #Determining number of locs for each bird, putting in new column n.locs and filtering for locs > n. Map functions apply a function to each element in a list.
  mutate(n.locs = map_dbl(indData, ~nrow(.))) %>% 
  filter(n.locs > 25) %>%
  #Making a track object. The locations we imported were just a snapshot of a location at a given time, while track objects will represent the movement of NOBOs over time. Map applies the function to each nested indData within the tibble.
  mutate(tracks = purrr::map(indData, function(x){
  #x <- rsfData$indData[[1]] This is a bugfinder. Leave muted unless needed.
    x %>%
      make_track(.x = easting,
                 .y = northing,
                 .t = DT,
                 crs = 26916)
    })) %>%
  mutate(kde = purrr::map(tracks, function(a){
    # a <- rsfData_summer2022$tracks[[2]]
    a %>%
      hr_kde() %>%
      hr_ud() %>%
      hr_isopleths(levels = c(.5,.95))
    })) %>%
  # hr_to_sf(kde, band_numb, tracks)
  mutate(indData = map2(tracks, kde, function(x,y){

      #Below we generate x random points within each NOBO's KDE home range.
      random_points(y, n = nrow(x) * 10, level=.95, presence = x) %>%
      #Extracting covariates (lcov types) of known and random points
      extract_covariates(distanceStack23)
  })) %>%
  #Unnesting data back into original rows.
  unnest(indData) 


rsfData_modified_summer2023 <- rsfData_summer2023 %>% 
  mutate(case = ifelse(case_ == T, 1, 0)) %>%
  mutate_at(vars(AG:WL), .funs = function(x){as.numeric(scale(x, center = T))}) 

```

Combining Summer 2022/23 data.
```{r}
#### Combining summer 2022/23 data ####
rsf_modified_2022_23 <- rbind(
  rsfData_modified_summer2022, rsfData_modified_summer2023
)
```


Summer 2023 Bayesian RSF: Looking at all age and sex classes
```{r}
#### Summer 2023  Bayesian: Looking at all age and sex classes. #### 

pop.rsf_summer2023 <- brm(
  
  #Modelling 'case' as a binomial variable (either 0 or 1...aka random or known points). Predictor variables are our land cover types, band number  is our random effect.
  case | trials(1) ~ AG + UM + NP_UB +NP_BU + PP_UB + PP_BU + HW + HP_UB + HP_BU + SS_UB + SS_BU + (1|band_numb),


  #Specifying our dataset
  data = rsfData_modified_summer2023,

  #Specifying a binomial model. 
  family = "binomial",

  #Setting our priors. We're using a fairly wide distribution (SD=5) centered around 0, which suggests we know little about the parameters before starting. I'm unsure if these prior distributions are appropriate.
  prior = c(
    set_prior("normal(0, 2)", class = "Intercept"), # Prior for the intercept. 
    set_prior("normal(0, 2)", class = "b"),         # Prior for the fixed effects (land cover types)
    set_prior("normal(0, 2)", class = "sd")         # Prior for the standard deviation (random effect...band_numb)
  ),

  #Specifying 10,000 iterations. A good balance of speed and accuracy.
  iter = 10000,

  #4 MCMC chains seems to be a common choice for robust modeling.
  chains = 4,
  
  seed = 123,
  
  warmup = 5000,
  
   #Adjusting adapt_delta... This tells sampler to be more careful in "trickier" parts of the model.
  control = list(adapt_delta = 0.9)
)


#Printing summary of results.
summary(pop.rsf_summer2023)

# Get the summary of the model
model_summary_summer2023 <- summary(pop.rsf_summer2023)


#Below we calculate our logs odds ratios.
# Extract the fixed effects (estimates) from the model summary
fixed_effects <- model_summary_summer2023$fixed

# Create a data frame from the fixed effects
coefficients_data <- as.data.frame(fixed_effects)

# Adding a column for the odds ratios by exponentiating the estimates
coefficients_data$OddsRatio <- exp(coefficients_data$Estimate)


# Displaying results
print(coefficients_data)

#Plotting coefficients and confidence intervals
# Add a new column to store the row names... seems to be the only way to filter out the intercept.
coefficients_data$Variable <- rownames(coefficients_data)

coefficients_data_no_intercept_23 <- coefficients_data %>%
  filter(Variable != "Intercept") %>% 
  clean_names()

#coefficients_data_no_intercept_23$year <- 2023

#coe2022_23 <- rbind(coefficients_data_no_intercept_22, coefficients_data_no_intercept_23)

# Filter out the intercept and plot
coefficients_data_no_intercept_23 %>%
  ggplot(aes(x = reorder(variable, -estimate), y = estimate)) +
  geom_point() +  # Plot the beta coefficient point estimates
  geom_errorbar(aes(ymin = l_95_percent_ci, ymax = u_95_percent_ci), width = 0.2) + 
  geom_hline(yintercept = 0, linetype = "dashed", color = "gray", linewidth = 0.5) +  
  coord_flip() +  # Flip the axis for better visibility
  xlab('Variable') +
  ylab('Beta Coefficient') +
  ggtitle('Summer 2023 Beta Coefficients') +
  theme_minimal()


# For the trace plots
color_scheme_set("brightblue")
mcmc_trace(pop.rsf_summer2023_bayes, 
           pars = c("b_Intercept", "b_AG", "b_UM", "b_WL", "b_NP_UB", "b_NP_BU", "b_PP_UB", "b_PP_BU", "b_HW", "b_HP_BU", "b_HP_UB", "b_SS_UB", "b_SS_BU"),
           n_warmup = 1000,
           facet_args = list(ncol = 2)) +
  xlab("Iteration") +
  ylab("Parameter Estimate (all birds")

# Get the posterior samples as a data frame (posterior package deprecated)...
posterior_draws <- as_draws_df(pop.rsf_summer2023)


#Selecting columns we want to plot, renaming
posterior_draws <- posterior_draws %>%
  as_tibble() %>%
  select(starts_with("b_"), -b_Intercept) %>%
  rename(AG = b_AG,
         HP_BU = b_HP_BU,
         HP_UB = b_HP_UB,
         HW = b_HW,
         NP_BU = b_NP_BU,
         NP_UB = b_NP_UB,
         PP_BU = b_PP_BU,
         PP_UB = b_PP_UB,
         SS_BU = b_SS_BU,
         SS_UB = b_SS_UB,
         UM = b_UM)

# Pivoting long, so we can facet wrap by land cover type
posterior_draws_long <- posterior_draws %>%
  pivot_longer(cols = AG:SS_BU, names_to = "landcover", values_to = "value")

# plotting density of posterior draws
ggplot(posterior_draws_long, aes(x = value, fill = landcover)) +
  geom_density(alpha = 0.7) +
  facet_wrap(~ landcover, scales = "free") +
  theme_minimal() +
  xlab("Posterior Distribution of Beta Coefficients") +
  ylab("Density") +
  ggtitle("Posterior Distributions of Summer 2023 Beta Coefficients") +
  geom_vline(xintercept = 0, linetype = "dashed", color = "black") +
  theme(legend.position = "none")

```

Summer 2022 AND 2023 Bayesian RSF: Looking at all age and sex classes
```{r}
#### Summer 2022 AND 2023; HP and PP burned #### Cannot include HP_BU and HP_UB in the same model (correlation). Same with PP_BU and PP_UB


pop.rsf_summer2022_23_bayes_bu <- brm(
  
  #Modelling 'case' as a binomial variable (either 0 or 1...aka random or known points). Predictor variables are our land cover types, band number  is our random effect.
  case | trials(1) ~ AG + UM + NP_UB + NP_BU + PP_BU + HW + HP_BU + SS_UB + SS_BU + (1|band_numb),


  #Specifying our dataset
  data = rsf_modified_2022_23,

  #Specifying a binomial model. 
  family = "binomial",

  #Setting our priors. We're using a fairly wide distribution (SD=5) centered around 0, which suggests we know little about the parameters before starting. I'm unsure if these prior distributions are appropriate.
  #prior = c(
    #set_prior("normal(0, 2)", class = "Intercept"), # Prior for the intercept. 
    #set_prior("normal(0, 2)", class = "b"),         # Prior for the fixed effects (land cover types)
    #set_prior("normal(0, 2)", class = "sd")         # Prior for the standard deviation (random effect...band_numb)
  #),

  #Specifying 10000 iterations. A good balance of speed and accuracy.
  iter = 4000,

  #4 MCMC chains seems to be a common choice for robust modeling.
  chains = 4,
  
  seed = 123,
  
  warmup = 2000,
  
   #Adjusting adapt_delta... This tells sampler to be more careful in "trickier" parts of the model.
  control = list(adapt_delta = 0.9)
)

#Printing summary of results.
summary(pop.rsf_summer2022_23_bayes_bu)

# Get the summary of the model
model_summary_summer2022_23_bayes_bu <- summary(pop.rsf_summer2022_23_bayes_bu)


#Below we calculate our logs odds ratios.
# Extract the fixed effects (estimates) from the model summary
fixed_effects <- model_summary_summer2022_23_bayes_bu$fixed

# Create a data frame from the fixed effects
coefficients_data <- as.data.frame(fixed_effects)

# Adding a column for the odds ratios by exponentiating the estimates
coefficients_data$OddsRatio <- exp(coefficients_data$Estimate)


# Displaying results
print(coefficients_data)

#Plotting coefficients and confidence intervals
# Add a new column to store the row names... seems to be the only way to filter out the intercept.
coefficients_data$Variable <- rownames(coefficients_data)

coefficients_data_no_intercept <- coefficients_data %>%
  filter(Variable != "Intercept")

coefficients_data_no_intercept_2022_2023 <- coefficients_data_no_intercept %>%
  clean_names()


#plotting beta coefficients and confidence levels
coefficients_data_no_intercept_2022_2023 %>%
  ggplot(aes(x = reorder(variable, -estimate), y = estimate)) +
  geom_point() + 
  geom_errorbar(aes(ymin = l_95_percent_ci, ymax = u_95_percent_ci), width = 0.2) + 
  geom_hline(yintercept = 0, linetype = "dashed", color = "gray", linewidth = 0.5) +  
  coord_flip() +  
  labs(x = "Land cover",
       y = "Beta Coefficient",
       title = "Beta coefficients: Combined Summer 2022/23 locations (HP and PP Burned)",
       subtitle = "Shown with 95% credible intervals") +
  theme_minimal()


# For the trace plots
color_scheme_set("brightblue")
mcmc_trace(pop.rsf_summer2022_23_bayes_bu, 
           pars = c("b_Intercept", "b_AG", "b_UM", "b_NP_UB", "b_NP_BU", "b_PP_BU", "b_HW", "b_HP_BU", "b_SS_UB", "b_SS_BU"),
           n_warmup = 5000,
           facet_args = list(ncol = 2)) +
  xlab("Iteration") +
  ylab("Parameter Estimate (all birds") +
  ggtitle("Trace Plot 2022/23 (SD = 2)")

# Get the posterior samples as a data frame (posterior package deprecated)...
posterior_draws_bu <- as_draws_df(pop.rsf_summer2022_23_bayes_bu)


#Selecting columns we want to plot, renaming
posterior_draws_bu <- posterior_draws_bu %>%
  as_tibble() %>%
  select(starts_with("b_"), -b_Intercept) %>%
  rename(AG = b_AG,
         HP_BU = b_HP_BU,
         HW = b_HW,
         NP_BU = b_NP_BU,
         NP_UB = b_NP_UB,
         PP_BU = b_PP_BU,
         SS_BU = b_SS_BU,
         SS_UB = b_SS_UB,
         UM = b_UM)

# Pivoting long, so we can facet wrap by land cover type
posterior_draws_long_bu <- posterior_draws_bu %>%
  pivot_longer(cols = AG:SS_BU, names_to = "landcover", values_to = "value")

# plotting density of posterior draws
posterior_draws_long_bu %>%
  ggplot(aes(x = value, fill = landcover)) +
  geom_density(alpha = 0.7, position = "jitter") +
  facet_wrap(~factor(landcover, levels = c("NP_BU", "SS_UB", "AG", "PP_BU", "SS_BU", "UM", "HW", "NP_UB", "HP_BU"))) +
  theme_minimal() +
  labs(x = "Posterior Distribution of Beta Coefficients",
      y = "Density", 
      title = "Posterior distributions of breeding season beta coefficients (HP and PP burned)",
      subtitle = "2022 and 2023 data") +
  geom_vline(xintercept = 0, linetype = "dashed", color = "black") +
  theme(legend.position = "none")
```

```{r}
#### 2022 AND 2023 data; HP and PP unburned ####
pop.rsf_summer2022_23_bayes_ub <- brm(
  
  #Modelling 'case' as a binomial variable (either 0 or 1...aka random or known points). Predictor variables are our land cover types, band number  is our random effect.
  case | trials(1) ~ AG + UM + NP_UB + NP_BU + PP_UB + HW + HP_UB + SS_UB + SS_BU + (1|band_numb),


  #Specifying our dataset
  data = rsf_modified_2022_23,

  #Specifying a binomial model. 
  family = "binomial",

  #Setting our priors. We're using a fairly wide distribution (SD=5) centered around 0, which suggests we know little about the parameters before starting. I'm unsure if these prior distributions are appropriate.
  prior = c(
    set_prior("normal(0, 2)", class = "Intercept"), # Prior for the intercept. 
    set_prior("normal(0, 2)", class = "b"),         # Prior for the fixed effect
    set_prior("normal(0, 2)", class = "sd")         # Prior for the random effect)
  ),

  #Specifying 10000 iterations. A good balance of speed and accuracy.
  iter = 4000,

  #4 MCMC chains seems to be a common choice for robust modeling.
  chains = 4,
  
  seed = 123,
  
  warmup = 2000,
  
   #Adjusting adapt_delta... This tells sampler to be more careful in "trickier" parts of the model.
  control = list(adapt_delta = 0.9)
)

#Printing summary of results.
summary(pop.rsf_summer2022_23_bayes_ub)

# Get the summary of the model
model_summary_summer2022_23_bayes_ub <- summary(pop.rsf_summer2022_23_bayes_ub)


#Below we calculate our logs odds ratios.
# Extract the fixed effects (estimates) from the model summary
fixed_effects <- model_summary_summer2022_23_bayes_ub$fixed

# Create a data frame from the fixed effects
coefficients_data <- as.data.frame(fixed_effects)

# Adding a column for the odds ratios by exponentiating the estimates
coefficients_data$OddsRatio <- exp(coefficients_data$Estimate)


# Displaying results
print(coefficients_data)

#Plotting coefficients and confidence intervals
# Add a new column to store the row names... seems to be the only way to filter out the intercept.
coefficients_data$Variable <- rownames(coefficients_data)

coefficients_data_no_intercept <- coefficients_data %>%
  filter(Variable != "Intercept")

coefficients_data_no_intercept_2022_2023 <- coefficients_data_no_intercept %>%
  clean_names()


#plotting beta coefficients and confidence levels
coefficients_data_no_intercept_2022_2023 %>%
  ggplot(aes(x = reorder(variable, -estimate), y = estimate)) +
  geom_point() + 
  geom_errorbar(aes(ymin = l_95_percent_ci, ymax = u_95_percent_ci), width = 0.2) + 
  geom_hline(yintercept = 0, linetype = "dashed", color = "gray", linewidth = 0.5) +  
  coord_flip() +  
  labs(x = "Land cover",
       y = "Beta Coefficient",
       title = "Beta coefficients: Combined Summer 2022/23 locations (HP and PP unburned)",
       subtitle = "Shown with 95% credible intervals") +
  theme_minimal()


# For the trace plots
color_scheme_set("brightblue")
mcmc_trace(pop.rsf_summer2022_23_bayes_ub, 
           pars = c("b_Intercept", "b_AG", "b_UM", "b_NP_UB", "b_NP_BU", "b_PP_UB", "b_HW", "b_HP_UB", "b_SS_UB", "b_SS_BU"),
           n_warmup = 5000,
           facet_args = list(ncol = 2)) +
  xlab("Iteration") +
  ylab("Parameter Estimate (all birds") +
  ggtitle("Trace Plot 2022/23 (SD = 2)")

# Get the posterior samples as a data frame (posterior package deprecated)...
posterior_draws_ub <- as_draws_df(pop.rsf_summer2022_23_bayes_ub)


#Selecting columns we want to plot, renaming
posterior_draws_ub <- posterior_draws_ub %>%
  as_tibble() %>%
  select(starts_with("b_"), -b_Intercept) %>%
  rename(AG = b_AG,
         HP_UB = b_HP_UB,
         HW = b_HW,
         NP_BU = b_NP_BU,
         NP_UB = b_NP_UB,
         PP_UB = b_PP_UB,
         SS_BU = b_SS_BU,
         SS_UB = b_SS_UB,
         UM = b_UM)

# Pivoting long, so we can facet wrap by land cover type
posterior_draws_long_ub <- posterior_draws_ub %>%
  pivot_longer(cols = AG:SS_BU, names_to = "landcover", values_to = "value")

# plotting density of posterior draws
posterior_draws_long_ub %>%
  ggplot(aes(x = value, fill = landcover)) +
  geom_density(alpha = 0.7, position = "jitter") +
  facet_wrap(~factor(landcover, levels = c("NP_BU", "AG", "SS_UB", "PP_UB", "HP_UB", "SS_BU", "UM", "NP_UB", "HW"))) +
  theme_minimal() +
  labs(x = "Posterior Distribution of Beta Coefficients",
      y = "Density", 
      title = "Posterior distributions of breeding season beta coefficients (HP and PP unburned)",
      subtitle = "2022 and 2023 data") +
  geom_vline(xintercept = 0, linetype = "dashed", color = "black") +
  theme(legend.position = "none")


```


```{r}
#### Averaging both models 2022/23 Handling correlations between HP_UB/BU and PP_UB/BU####
#This has PP and HP as burned
pop.rsf_summer2022_23_bayes_bu <- brm(
  
  #Modelling 'case' as a binomial variable (either 0 or 1...aka random or known points). Predictor variables are our land cover types, band number  is our random effect.
  case | trials(1) ~ AG + UM + NP_UB + NP_BU + PP_BU + HW + HP_BU + SS_UB + SS_BU + (1|band_numb),


  #Specifying our dataset
  data = rsf_modified_2022_23,

  #Specifying a binomial model. 
  family = "binomial",

  #Setting our priors. We're using a fairly wide distribution (SD=5) centered around 0, which suggests we know little about the parameters before starting. I'm unsure if these prior distributions are appropriate.
  #prior = c(
    #set_prior("normal(0, 2)", class = "Intercept"), # Prior for the intercept. 
    #set_prior("normal(0, 2)", class = "b"),         # Prior for the fixed effects (land cover types)
    #set_prior("normal(0, 2)", class = "sd")         # Prior for the standard deviation (random effect...band_numb)
  #),

  #Specifying 10000 iterations. A good balance of speed and accuracy.
  iter = 4000,

  #4 MCMC chains seems to be a common choice for robust modeling.
  chains = 4,
  
  seed = 123,
  
  warmup = 2000,
  
   #Adjusting adapt_delta... This tells sampler to be more careful in "trickier" parts of the model.
  control = list(adapt_delta = 0.9)
)

#This model has HP and PP as UB
pop.rsf_summer2022_23_bayes_ub <- brm(
  
  #Modelling 'case' as a binomial variable (either 0 or 1...aka random or known points). Predictor variables are our land cover types, band number  is our random effect.
  case | trials(1) ~ AG + UM + NP_UB + NP_BU + PP_UB + HW + HP_UB + SS_UB + SS_BU + (1|band_numb),


  #Specifying our dataset
  data = rsf_modified_2022_23,

  #Specifying a binomial model. 
  family = "binomial",

  #Setting our priors. We're using a fairly wide distribution (SD=5) centered around 0, which suggests we know little about the parameters before starting. I'm unsure if these prior distributions are appropriate.
  prior = c(
    set_prior("normal(0, 2)", class = "Intercept"), # Prior for the intercept. 
    set_prior("normal(0, 2)", class = "b"),         # Prior for the fixed effect
    set_prior("normal(0, 2)", class = "sd")         # Prior for the random effect)
  ),

  #Specifying 10000 iterations. A good balance of speed and accuracy.
  iter = 4000,

  #4 MCMC chains seems to be a common choice for robust modeling.
  chains = 4,
  
  seed = 123,
  
  warmup = 2000,
  
   #Adjusting adapt_delta... This tells sampler to be more careful in "trickier" parts of the model.
  control = list(adapt_delta = 0.9)
)

mcmc_areas(posterior_draws_ub, pars = c("b_AG",
         "b_HP_UB",
         "b_HW",
         "b_NP_BU",
         "b_NP_UB",
         "b_PP_UB",
         "b_SS_BU",
         "b_SS_UB",
         "b_UM"), 
         prob = 0.95)



```
 
```{r}
#### 2022/23 males ####
pop.rsf_summer2022_23_males <- brm(
  
  #Modelling 'case' as a binomial variable (either 0 or 1...aka random or known points). Predictor variables are our land cover types, band number  is our random effect.
  case | trials(1) ~ AG + UM + NP_UB + NP_BU + PP_UB + PP_BU + HW + HP_UB + HP_BU + SS_UB + SS_BU + (1|band_numb),


  #Specifying our dataset
  data = rsf_modified_2022_23 %>%
    dplyr::filter(sex == "M"),

  #Specifying a binomial model. 
  family = "binomial",

  #Setting our priors. We're using a fairly wide distribution (SD=5) centered around 0, which suggests we know little about the parameters before starting. I'm unsure if these prior distributions are appropriate.
  prior = c(
    set_prior("normal(0, 2)", class = "Intercept"), # Prior for the intercept. 
    set_prior("normal(0, 2)", class = "b"),         # Prior for the fixed effects (land cover types)
    set_prior("normal(0, 2)", class = "sd")         # Prior for the standard deviation (random effect...band_numb)
  ),

  #Specifying 10000 iterations. A good balance of speed and accuracy.
  iter = 5000,

  #4 MCMC chains seems to be a common choice for robust modeling.
  chains = 4,
  
   #Adjusting adapt_delta... This tells sampler to be more careful in "trickier" parts of the model.
  control = list(adapt_delta = 0.9)
)

#Printing summary of results.
summary(pop.rsf_summer2022_23_males)

# Get the summary of the model
model_summary_summer2022_23_males <- summary(pop.rsf_summer2022_23_males)


#Below we calculate our logs odds ratios.
# Extract the fixed effects (estimates) from the model summary
fixed_effects <- model_summary_summer2022_23_males$fixed

# Create a data frame from the fixed effects
coefficients_data <- as.data.frame(fixed_effects)

# Adding a column for the odds ratios by exponentiating the estimates
coefficients_data$OddsRatio <- exp(coefficients_data$Estimate)

# Adding columns for the lower and upper limits of the 95% confidence interval for the odds ratios
coefficients_data$l_95_CI_OR <- exp(coefficients_data$Estimate - 1.96 * coefficients_data$Est.Error)
coefficients_data$u_95_CI_OR <- exp(coefficients_data$Estimate + 1.96 * coefficients_data$Est.Error)

# Displaying results
print(coefficients_data)

#Plotting coefficients and confidence intervals
# Add a new column to store the row names... seems to be the only way to filter out the intercept.
coefficients_data$Variable <- rownames(coefficients_data)

coefficients_data_no_intercept <- coefficients_data %>%
  filter(Variable != "Intercept")

coefficients_data_no_intercept_2022_2023_males <- coefficients_data_no_intercept



# Filter out the intercept and plot
coefficients_data_no_intercept_2022_2023_males %>%
  ggplot(aes(x = reorder(Variable, -Estimate), y = Estimate)) +
  geom_point() +  # Plot the beta coefficient point estimates
  geom_errorbar(aes(ymin = Estimate - 1.96 * Est.Error, ymax = Estimate + 1.96 * Est.Error), width = 0.2) + 
  geom_hline(yintercept = 0, linetype = "dashed", color = "gray", linewidth = 0.5) +  
  coord_flip() +  # Flip the axis for better visibility
  xlab('Land cover') +
  ylab('Beta Coefficient') +
  ggtitle('Beta coefficients (males)') +
  theme_minimal()


# For the trace plots
color_scheme_set("brightblue")
mcmc_trace(pop.rsf_summer2022_23_males, 
           pars = c("b_Intercept", "b_AG", "b_UM", "b_NP_UB", "b_NP_BU", "b_PP_UB", "b_PP_BU", "b_HW", "b_HP_BU", "b_HP_UB", "b_SS_UB", "b_SS_BU"),
           n_warmup = 1000,
           facet_args = list(ncol = 2)) +
  xlab("Iteration") +
  ylab("Parameter Estimate (all birds") +
  ggtitle("Trace Plot 2022/23 (SD = 2)")
```

Summer 2022/23 females
```{r}
####
pop.rsf_summer2022_23_females <- brm(
  
  #Modelling 'case' as a binomial variable (either 0 or 1...aka random or known points). Predictor variables are our land cover types, band number  is our random effect.
  case | trials(1) ~ AG + UM + NP_UB + NP_BU + PP_UB + PP_BU + HW + HP_UB + HP_BU + SS_UB + SS_BU + (1|band_numb),


  #Specifying our dataset
  data = rsf_modified_2022_23 %>%
    dplyr::filter(sex == "F"),

  #Specifying a binomial model. 
  family = "binomial",

  #Setting our priors. We're using a fairly wide distribution (SD=5) centered around 0, which suggests we know little about the parameters before starting. I'm unsure if these prior distributions are appropriate.
  prior = c(
    set_prior("normal(0, 2)", class = "Intercept"), # Prior for the intercept. 
    set_prior("normal(0, 2)", class = "b"),         # Prior for the fixed effects (land cover types)
    set_prior("normal(0, 2)", class = "sd")         # Prior for the standard deviation (random effect...band_numb)
  ),

  #Specifying 10000 iterations. A good balance of speed and accuracy.
  iter = 10000,

  #4 MCMC chains seems to be a common choice for robust modeling.
  chains = 4,
  
   #Adjusting adapt_delta... This tells sampler to be more careful in "trickier" parts of the model.
  control = list(adapt_delta = 0.9)
)

#Printing summary of results.
summary(pop.rsf_summer2022_23_females)

# Get the summary of the model
model_summary_summer2022_23_females <- summary(pop.rsf_summer2022_23_females)


#Below we calculate our logs odds ratios.
# Extract the fixed effects (estimates) from the model summary
fixed_effects <- model_summary_summer2022_23_females$fixed

# Create a data frame from the fixed effects
coefficients_data <- as.data.frame(fixed_effects)

# Adding a column for the odds ratios by exponentiating the estimates
coefficients_data$OddsRatio <- exp(coefficients_data$Estimate)

# Adding columns for the lower and upper limits of the 95% confidence interval for the odds ratios
coefficients_data$l_95_CI_OR <- exp(coefficients_data$Estimate - 1.96 * coefficients_data$Est.Error)
coefficients_data$u_95_CI_OR <- exp(coefficients_data$Estimate + 1.96 * coefficients_data$Est.Error)

# Displaying results
print(coefficients_data)

#Plotting coefficients and confidence intervals
# Add a new column to store the row names... seems to be the only way to filter out the intercept.
coefficients_data$Variable <- rownames(coefficients_data)

coefficients_data_no_intercept <- coefficients_data %>%
  filter(Variable != "Intercept")

coefficients_data_no_intercept_2022_2023_females <- coefficients_data_no_intercept



# Filter out the intercept and plot
coefficients_data_no_intercept_2022_2023_males %>%
  ggplot(aes(x = reorder(Variable, -Estimate), y = Estimate)) +
  geom_point() +  # Plot the beta coefficient point estimates
  geom_errorbar(aes(ymin = Estimate - 1.96 * Est.Error, ymax = Estimate + 1.96 * Est.Error), width = 0.2) + 
  geom_hline(yintercept = 0, linetype = "dashed", color = "gray", linewidth = 0.5) +  
  coord_flip() +  # Flip the axis for better visibility
  xlab('Land cover') +
  ylab('Beta Coefficient') +
  ggtitle('Beta coefficients (males)') +
  theme_minimal()


# For the trace plots
color_scheme_set("brightblue")
mcmc_trace(pop.rsf_summer2022_23_bayes, 
           pars = c("b_Intercept", "b_AG", "b_UM", "b_NP_UB", "b_NP_BU", "b_PP_UB", "b_PP_BU", "b_HW", "b_HP_BU", "b_HP_UB", "b_SS_UB", "b_SS_BU"),
           n_warmup = 1000,
           facet_args = list(ncol = 2)) +
  xlab("Iteration") +
  ylab("Parameter Estimate (all birds") +
  ggtitle("Trace Plot 2022/23 (SD = 2)")
```

```{r}
#Importing "fate" spreadsheet, which has sex and age-class associated with each band number (unlike daily locations data)
fate <- rio::import("/Volumes/Samsung_T5/BOBWHITE_DATA/Clean/Fate/fate_all_birds_clean4880EDIT.xlsx", setclass = "tibble") %>%
  clean_names() 

#Adding sex to each observation/random location.
rsf_modified_2022_23 <- rsf_modified_2022_23 %>%
  left_join(fate %>% select(band_numb, sex), by = "band_numb")

rsf_22_23_males <- rsf_modified_2022_23 %>%
  dplyr::filter(sex == "M")

rsf_22_23_females <- rsf_modified_2022_23 %>%
  dplyr::filter(sex == "F")

```

Burned/unburned/never burned
```{r}
##### Looking at burned/unburned/neverburned #### 

habitat <- raster::shapefile("/Volumes/Samsung_T5/BOBWHITE_DATA/Maps/2023_LCov_BurnStatus/2023_LCOV_BurnStatus_V2.shp")

habitat$LCOV_MGMT2 <- gsub("AG", "n_b", habitat$LCOV_MGMT2)
habitat$LCOV_MGMT2 <- gsub("WL", "n_b", habitat$LCOV_MGMT2)
habitat$LCOV_MGMT2 <- gsub("UM", "n_b", habitat$LCOV_MGMT2)
habitat$LCOV_MGMT2 <- gsub("HW", "n_b", habitat$LCOV_MGMT2)
habitat$LCOV_MGMT2 <- gsub("NP_UB", "ub", habitat$LCOV_MGMT2)
habitat$LCOV_MGMT2 <- gsub("PP_UB", "ub", habitat$LCOV_MGMT2)
habitat$LCOV_MGMT2 <- gsub("HW_UB", "ub", habitat$LCOV_MGMT2)
habitat$LCOV_MGMT2 <- gsub("SS_UB", "ub", habitat$LCOV_MGMT2)
habitat$LCOV_MGMT2 <- gsub("HP_UB", "ub", habitat$LCOV_MGMT2)
habitat$LCOV_MGMT2 <- gsub("NP_BU", "bu", habitat$LCOV_MGMT2)
habitat$LCOV_MGMT2 <- gsub("PP_BU", "bu", habitat$LCOV_MGMT2)
habitat$LCOV_MGMT2 <- gsub("HW_BU", "bu", habitat$LCOV_MGMT2)
habitat$LCOV_MGMT2 <- gsub("SS_BU", "bu", habitat$LCOV_MGMT2)
habitat$LCOV_MGMT2 <- gsub("HP_BU", "bu", habitat$LCOV_MGMT2)



#Let's count the land cover types.
habitat %>%
  as.data.frame() %>%
  count(LCOV_MGMT2, sort = T) %>%
  print()


#Checking the extent of the bounding box.
extent(habitat@bbox)

#Creating a column that treats land cover (LCOV_MGMT2) types as a numeric factor.
habitat$hab.factor <- as.numeric(as.factor(habitat$LCOV_MGMT2))

#Viewing levels (landc over types aka HABITAT)
levels(as.factor(habitat$LCOV_MGMT2))

#Quick look
#head(habitat)

#Creating template raster. Here, we specify crs, extent, and resolution of our raster. For some reason, model won't run with a resolution below 3 meters. This shouldn't be an issues w telemetry data, as we want a larger raster cell size.
template.raster <- raster(crs = habitat@proj4string,
       ext = extent(habitat@bbox),
       res = 10)


#Rasterizing the map of study site, filling with land cover type (as numeric factors). Pretty sure this is unnecessary, as it is handled in our for() loop later.
habitat.raster <- rasterize(x = habitat,
                            y = template.raster,
                            field = habitat$hab.factor)

#plotting habitat raster habitat raster
plot(habitat.raster)
#windows()

#Viewing habitat shapefile. Land cover types here are now numeric values.
plot(habitat, col = as.factor(habitat$LCOV_MGMT2))
```

```{r}
#### Creating raster stack for burned/unburned #### 

#Assigning numeric land cover types to "habitats" as levels. "habitat" is our shapefile, while habitat.raster is the rasterized version of our shapefile. Unsure if it matters that we assign levels from the shapefile version.
habitats <- levels(as.factor(habitat$LCOV_MGMT2))

#Creating a raster list. This will be used in our for loop.
rasterList <- list()

#Creating our for loop. This creates a distance-based raster layer for each of our seven lcov types.
#Each raster cell in a given lcov layer will hold the distance to the nearest respective lcov type. This value will be 0 if the cell is within the lcov type. 
for(i in 1:length(habitats)){   #We'll loop through each lcov type in "habitats".
  # i <- 1 (debugger, leave muted unless needed)
  #for each layer in the stack, we create a subset called "hab" that only contains that respective lcov type.
  hab <- subset(habitat, LCOV_MGMT2 == habitats[i])   
  #assigning a value of 1 to each location in hab in a new column called field. So, in ag layer, each cell in ag$field = 1.
  hab$field <- 1 
  # plot(hab) Leave muted. (unnecessary unless you want to plot each layer as the loop runs) 
  #We rasterize hab subset in each stack, based on raster layer. We transfer values from hab$field. Lcov types outside of the respective layer (background) are assigned NA. We calc distance from each cell to nearest respective lcov type, creating a dist-based raster layer.
  rasterList[[i]] <- distance(rasterize(hab, template.raster, field = hab$field, background = NA))
  #Creating list of dist-based raster layers, naming each element after corresponding lcov type.
  names(rasterList)[i] <- habitats[i]
}

#This extracts the extent of each raster layer in our list.
lapply(rasterList, extent)

#Now we stack our raster layers.
distanceStack23bu <- stack(rasterList)

#Plotting our seven distance-based raster layers.
plot(distanceStack23bu)
```

```{r}
Summer_2023_locs <- rio::import("/Volumes/Samsung_T5/R_projects/_THESIS/Chapter1/S23locsUnweighted.xlsx", setclass = "tibble") %>%
  clean_names() %>%
  filter(status != "D",
         status != "N") %>%
  mutate(date = lubridate::as_date(date)) %>%
  filter(date > "2023-04-17",
         date < "2023-08-06")

```

```{r}
#### Creating summer 2022 rsf tibble for burned/unburned ####
#Creating tibble for RSF, which includes random and known points for each NOBO within kde home range, as well as extracted covariates.  
rsfData_summer2023_burned_unburned <- Summer_2023_locs %>% 
  #Converting it to a tibble
  as_tibble() %>%
   #Removing timezone offset, letter "T", creating DT.GMT column by converting to POSIX format, creating DT column by subtracting 5 hrs (time zones)
  mutate(DT.chr = gsub("-05:00","",gsub("T"," ",date_created)),
         DT.GMT = as.POSIXct(DT.chr, format = "%Y-%m-%d %H:%M:%S", tz = "GMT"),
         DT = DT.GMT-lubridate::hours(5)) %>% 
  #Nesting by band number into new column called indData.
  nest(indData = !band_numb) %>% 
  #Determining number of locs for each bird, putting in new column n.locs and filtering for locs > n. Map functions apply a function to each element in a list.
  mutate(n.locs = map_dbl(indData, ~nrow(.))) %>% 
  filter(n.locs > 30) %>%
  #Making a track object. The locations we imported were just a snapshot of a location at a given time, while track objects will represent the movement of NOBOs over time. Map applies the function to each nested indData within the tibble (it generated KDE homeranges, UDs and isopleths for each bird).
  mutate(tracks = purrr::map(indData, function(x){
  #x <- rsfData$indData[[1]] This is a bugfinder. Leave muted unless needed.
    x %>%
      make_track(.x = easting,
                 .y = northing,
                 .t = DT,
                 crs = 26916)
    })) %>%
  mutate(kde = purrr::map(tracks, function(a){
    # a <- rsfData_summer2022$tracks[[2]]
    a %>%
      hr_kde() %>%
      hr_ud() %>%
      hr_isopleths(levels = c(.5,.95))
    })) %>%
  # hr_to_sf(kde, band_numb, tracks)
  mutate(indData = map2(tracks, kde, function(x,y){
      #Below we generate x random points within each NOBO's KDE home range (95% UD).
      random_points(y, n = nrow(x) * 10, level=.95, presence = x) %>%
      #Extracting covariates (lcov types) of known and random points
      extract_covariates(distanceStack23bu)
  })) %>%
  #Unnesting data back into original rows.
  unnest(indData) 

#Creating column "case" where TRUEs (known points) have value of 1, and Falses (random points) have value of 0. Standardizing our covariates around the center.
rsfData_modified_summer2023_burned_unburned <- rsfData_summer2023_burned_unburned  %>% 
  mutate(case = ifelse(case_ == T, 1, 0)) %>%
  mutate_at(vars(bu:ub), .funs = function(x){as.numeric(scale(x, center = T))}) 

#### running bayes linear regression rsf ####
pop.rsf_summer2023_bayes_burned_unburned <- brm(
  
  #Modelling 'case' as a binomial variable (either 0 or 1...aka random or known points). Predictor variables are our land cover types, band number  is our random effect.
  case | trials(1) ~ bu + n_b + ub + (1|band_numb),


  #Specifying our dataset
  data = rsfData_modified_summer2023_burned_unburned,

  #Specifying a binomial model. 
  family = "binomial",

  #Setting our priors. We're using a fairly wide distribution (SD=5) centered around 0, which suggests we know little about the parameters before starting. I'm unsure if these prior distributions are appropriate.
  prior = c(
    set_prior("normal(0, 2)", class = "Intercept"), # Prior for the intercept. 
    set_prior("normal(0, 2)", class = "b"),         # Prior for the fixed effects (land cover types)
    set_prior("normal(0, 2)", class = "sd")         # Prior for the standard deviation (random effect...band_numb)
  ),

  #Specifying 10000 iterations. A good balance of speed and accuracy.
  iter = 10000,

  #4 MCMC chains seems to be a common choice for robust modeling.
  chains = 4,
  
   #Adjusting adapt_delta... This tells sampler to be more careful in "trickier" parts of the model.
  control = list(adapt_delta = 0.9)
)

#Printing summary of results.
summary(pop.rsf_summer2023_bayes_burned_unburned)

# Get the summary of the model
model_summary_summer2023_bayes_burned_unburned <- summary(pop.rsf_summer2023_bayes_burned_unburned)


#Below we calculate our logs odds ratios.
# Extract the fixed effects (estimates) from the model summary
fixed_effects <- model_summary_summer2023_bayes_burned_unburned$fixed

# Create a data frame from the fixed effects
coefficients_data <- as.data.frame(fixed_effects)

# Adding a column for the odds ratios by exponentiating the estimates
coefficients_data$OddsRatio <- exp(coefficients_data$Estimate)

# Adding columns for the lower and upper limits of the 95% confidence interval for the odds ratios
coefficients_data$l_95_CI_OR <- exp(coefficients_data$Estimate - 1.96 * coefficients_data$Est.Error)
coefficients_data$u_95_CI_OR <- exp(coefficients_data$Estimate + 1.96 * coefficients_data$Est.Error)

# Displaying results
print(coefficients_data)

#Plotting coefficients and confidence intervals
# Add a new column to store the row names... seems to be the only way to filter out the intercept.
coefficients_data$Variable <- rownames(coefficients_data)

coefficients_data_no_intercept <- coefficients_data %>%
  filter(Variable != "Intercept")

# Filter out the intercept and plot
coefficients_data_no_intercept %>%
  ggplot(aes(x = reorder(Variable, -Estimate), y = Estimate)) +
  geom_point() +  # Plot the beta coefficient point estimates
  geom_errorbar(aes(ymin = Estimate - 1.96 * Est.Error, ymax = Estimate + 1.96 * Est.Error), width = 0.2) + 
  geom_hline(yintercept = 0, linetype = "dashed", color = "gray", linewidth = 0.5) +  
  coord_flip() +  # Flip the axis for better visibility
  xlab('Variable') +
  ylab('Beta Coefficient') +
  ggtitle('Beta coefficients: burned and unburned areas') +
  theme_minimal()


# For the trace plots
color_scheme_set("brightblue")
mcmc_trace(pop.rsf_summer2023_bayes_burned_unburned, 
           pars = c("b_Intercept", "b_ub", "b_bu", "b_n_b"),
           n_warmup = 1000,
           facet_args = list(ncol = 2)) +
  xlab("Iteration") +
  ylab("Parameter Estimate (Burned vs Unburned)")


```

Looking at burned/unburned for combined 2022/23 data

```{r}
#### Combining 2022/23 data ####
rsf_bu_ub_22_23 <- rbind(rsfData_modified_summer2022_burned_unburned, rsfData_modified_summer2023_burned_unburned)
```

```{r}

#### running bayes linear regression rsf 22_23 burned/unburned ####
rsf22_23_bayes_burned_unburned <- brm(
  
  #Modelling 'case' as a binomial variable (either 0 or 1...aka random or known points). Predictor variables are our land cover types, band number  is our random effect.
  case | trials(1) ~ bu + n_b + ub + (1|band_numb),


  #Specifying our dataset
  data = rsf_bu_ub_22_23,

  #Specifying a binomial model. 
  family = "binomial",

  #Setting our priors. We're using a fairly wide distribution (SD=5) centered around 0, which suggests we know little about the parameters before starting. I'm unsure if these prior distributions are appropriate.
  prior = c(
    set_prior("normal(0, 2)", class = "Intercept"), # Prior for the intercept. 
    set_prior("normal(0, 2)", class = "b"),         # Prior for the fixed effects (land cover types)
    set_prior("normal(0, 2)", class = "sd")         # Prior for the standard deviation (random effect...band_numb)
  ),

  #Specifying 10000 iterations. A good balance of speed and accuracy.
  iter = 10000,

  #4 MCMC chains seems to be a common choice for robust modeling.
  chains = 4,
  
   #Adjusting adapt_delta... This tells sampler to be more careful in "trickier" parts of the model.
  control = list(adapt_delta = 0.9),
  
  seed = 123,
  
  warmup = 5000
)

#Printing summary of results.
summary(rsf22_23_bayes_burned_unburned)

# Get the summary of the model
model_summary_summer2022_23_bayes_burned_unburned <- summary(rsf22_23_bayes_burned_unburned)


#Below we calculate our logs odds ratios.
# Extract the fixed effects (estimates) from the model summary
fixed_effects <- model_summary_summer2022_23_bayes_burned_unburned$fixed

# Create a data frame from the fixed effects
coefficients_data <- as.data.frame(fixed_effects)

# Adding a column for the odds ratios by exponentiating the estimates
coefficients_data$OddsRatio <- exp(coefficients_data$Estimate)


# Displaying results
print(coefficients_data)

#Plotting coefficients and confidence intervals
# Add a new column to store the row names... seems to be the only way to filter out the intercept.
coefficients_data$Variable <- rownames(coefficients_data)

coefficients_data_no_intercept <- coefficients_data %>%
  filter(Variable != "Intercept") %>% 
  clean_names()

# Filter out the intercept and plot
coefficients_data_no_intercept %>%
  ggplot(aes(x = reorder(variable, -estimate), y = estimate)) +
  geom_point() +  # Plot the beta coefficient point estimates
  geom_errorbar(aes(ymin = l_95_percent_ci, ymax = u_95_percent_ci), width = 0.2) + 
  geom_hline(yintercept = 0, linetype = "dashed", color = "gray", linewidth = 0.5) +  
  coord_flip() +  # Flip the axis for better visibility
  labs(x = "Beta cofficient",
       y = "Variable",
       title = "Beta coefficients of burned, unburned, and never burned land cover",
       subtitle = "2022/23 breeding season locations") +
  theme_minimal()


# For the trace plots
color_scheme_set("brightblue")
mcmc_trace(rsf22_23_bayes_burned_unburned, 
           pars = c("b_Intercept", "b_ub", "b_bu", "b_n_b"),
           n_warmup = 1000,
           facet_args = list(ncol = 2)) +
  xlab("Iteration") +
  ylab("Parameter Estimate (Burned vs Unburned)")

# Get the posterior samples as a data frame (posterior package deprecated)...
posterior_draws_bu <- as_draws_df(rsf22_23_bayes_burned_unburned)


#Selecting columns we want to plot, renaming
posterior_draws <- posterior_draws_bu %>%
  as_tibble() %>%
  select(starts_with("b_"), -b_Intercept) %>%
  rename(BU = b_bu,
         UB = b_ub,
         NB = b_n_b)
         
# Pivoting long, so we can facet wrap by land cover type
posterior_draws_long <- posterior_draws %>%
  pivot_longer(cols = BU:UB, names_to = "landcover", values_to = "value")

# plotting density of posterior draws
posterior_draws_long %>%
  ggplot(aes(x = value, fill = landcover)) +
  geom_density(alpha = 0.7) +
  facet_wrap(~factor(landcover, levels = c("UB", "BU", "NB"))) +
  theme_minimal() +
  xlab("Posterior Distribution of Beta Coefficients") +
  ylab("Density") +
  ggtitle("Posterior Distributions of Summer 2023 Beta Coefficients") +
  geom_vline(xintercept = 0, linetype = "dashed", color = "black") +
  theme(legend.position = "none")

```

Predict plots       
```{r}
#### Playing with predict plots ####

pred_model <-  brm(
  
  #Modelling 'case' as a binomial variable (either 0 or 1...aka random or known points). Predictor variables are our land cover types, band number  is our random effect.
  case | trials(1) ~ AG + UM + NP_UB +NP_BU + PP_UB + PP_BU + HW + HP_UB + HP_BU + SS_UB + SS_BU,


  #Specifying our dataset
  data = rsf_modified_2022_23,

  #Specifying a binomial model. 
  family = "binomial",

  #Setting our priors. We're using a fairly wide distribution (SD=5) centered around 0, which suggests we know little about the parameters before starting. I'm unsure if these prior distributions are appropriate.
  prior = c(
    set_prior("normal(0, 2)", class = "Intercept"), # Prior for the intercept. 
    set_prior("normal(0, 2)", class = "b")),         # Prior for the fixed effects (land cover types)
    #set_prior("normal(0, 2)", class = "sd")         # Prior for the standard deviation (random effect...band_numb)
  #),

  #Specifying 10,000 iterations. A good balance of speed and accuracy.
  iter = 10000,

  #4 MCMC chains seems to be a common choice for robust modeling.
  chains = 4,
  
  seed = 123,
  
  warmup = 5000,
  
   #Adjusting adapt_delta... This tells sampler to be more careful in "trickier" parts of the model.
  control = list(adapt_delta = 0.9)
)

#Let's start with NP_BU
np_bu_values <- seq(min(rsf_modified_2022_23$NP_BU, na.rm = TRUE), 
                 max(rsf_modified_2022_23$NP_BU, na.rm = TRUE), 
                 length.out = 1000)

np_bu_df <- data.frame(
  NP_BU = np_bu_values,
  AG = rep(mean(rsf_modified_2022_23$AG), length(np_bu_values)),
  HP_BU = rep(mean(rsf_modified_2022_23$HP_BU), length(np_bu_values)),
  HP_UB = rep(mean(rsf_modified_2022_23$HP_UB), length(np_bu_values)),
  HW = rep(mean(rsf_modified_2022_23$HW), length(np_bu_values)),
  NP_UB = rep(mean(rsf_modified_2022_23$NP_UB), length(np_bu_values)),
  PP_BU = rep(mean(rsf_modified_2022_23$PP_BU), length(np_bu_values)),
  PP_UB = rep(mean(rsf_modified_2022_23$PP_UB), length(np_bu_values)),
  SS_BU = rep(mean(rsf_modified_2022_23$SS_BU), length(np_bu_values)),
  SS_UB = rep(mean(rsf_modified_2022_23$SS_UB), length(np_bu_values)),
  UM = rep(mean(rsf_modified_2022_23$UM), length(np_bu_values))
  )

 View(np_bu_df)
 
 pred_npbu <- as.data.frame(posterior_predict(pred_model, newdata = np_bu_df, type = "response", se.fit = TRUE))
 
 
 pred_npbu <- pred_npbu %>%
   clean_names()
 
 pred_npbu$np_bu_values <- np_bu_values
 
pred_npbu$lowCI = (pred_npbu$estimate - (1.96 * pred_npbu$est_error))
pred_npbu$upperCI = (pred_npbu$estimate + (1.96 * pred_npbu$est_error))

pred_npbu %>%
  ggplot(aes(np_bu_values, estimate)) +
  geom_line() +
  geom_ribbon(aes(ymin = lowCI, ymax = upperCI), fill = "blue", alpha = 0.1) +
  labs(x = "Distance from Natural Pine Burned (centered and scaled)",
       y = "Predicted probability of use",
       title = "Predicted probability of use of NP_BU") +
  scale_y_continuous(labels = percent)


#Now let's try HP_BU
hp_bu_values <- seq(min(rsf_modified_2022_23$HP_BU, na.rm = TRUE), 
                 max(rsf_modified_2022_23$HP_BU, na.rm = TRUE), 
                 length.out = 1000)

hp_bu_df <- data.frame(
  NP_BU = rep(mean(rsf_modified_2022_23$NP_BU), length(hp_bu_values)),
  AG = rep(mean(rsf_modified_2022_23$AG), length(hp_bu_values)),
  HP_BU = hp_bu_values,
  HP_UB = rep(mean(rsf_modified_2022_23$HP_UB), length(hp_bu_values)),
  HW = rep(mean(rsf_modified_2022_23$HW), length(hp_bu_values)),
  NP_UB = rep(mean(rsf_modified_2022_23$NP_UB), length(hp_bu_values)),
  PP_BU = rep(mean(rsf_modified_2022_23$PP_BU), length(hp_bu_values)),
  PP_UB = rep(mean(rsf_modified_2022_23$PP_UB), length(hp_bu_values)),
  SS_BU = rep(mean(rsf_modified_2022_23$SS_BU), length(hp_bu_values)),
  SS_UB = rep(mean(rsf_modified_2022_23$SS_UB), length(hp_bu_values)),
  UM = rep(mean(rsf_modified_2022_23$UM), length(hp_bu_values))
  )

 View(hp_bu_df)
 
 pred_hpbu <- as.data.frame(predict(pred_model, newdata = hp_bu_df, type = "response", se.fit = TRUE))
 
 
 pred_hpbu <- pred_hpbu %>%
   clean_names()
 
 pred_hpbu$hp_bu_values <- hp_bu_values
 
pred_hpbu$lowCI = (pred_hpbu$estimate - (1.96 *pred_hpbu$est_error))
pred_hpbu$upperCI = (pred_hpbu$estimate + (1.96 *pred_hpbu$est_error))

pred_hpbu %>%
  ggplot(aes(hp_bu_values, estimate)) +
  geom_line() +
  geom_ribbon(aes(ymin = lowCI, ymax = upperCI), fill = "blue", alpha = 0.1) +
  labs(x = "Distance from Hardwood Pine Burned (centered and scaled)",
       y = "Predicted probability of use",
       title = "Predicted probability of use of HP_BU") +
  scale_y_continuous(labels = percent)


```

Determining sizes of KDE home ranges
```{r}
#### Size of KDE home ranges ####

#Making list of birds with 30+ observations
samp_list_23 <- Summer_2023_locs %>%
  group_by(band_numb) %>%
  summarize(n = n()) %>%
  arrange(n) %>%
  filter(n > 30) %>%
  dplyr::select(band_numb) %>%
  pull(band_numb)

#Filtering spreadsheet; only including birds from samp_list --> "SampleSize_winter_locs" 
Summer_2023_locs_KDE <- Summer_2023_locs %>%
  filter(band_numb %in% samp_list_23) %>%
  droplevels()


#Counting obs for each bird. 
Summer_2023_locs_KDE %>%
  count(band_numb, sort = TRUE, name = "obs")


#Setting projection and CRS to NAD 1983 16N
prj <- '+init=epsg:26916'
CRS("+init=epsg:26916")



#Creating spatial points data frame.
Summer_2023_locs_KDE_SPDF <- SpatialPointsDataFrame(coordinates(cbind(Summer_2023_locs_KDE$easting, Summer_2023_locs_KDE$northing)), data = Summer_2023_locs_KDE, proj4string = CRS(prj))

Summer_2023_locs_KDE_SPDF %>%
  str()

#Verifying band_numb is in row 7.
Summer_2023_locs_KDE_SPDF[,7]

#Creating KDE HRs
kde_hr_sum_23 <- kernelUD(Summer_2023_locs_KDE_SPDF[,7])


#Side-by-side image of all bobwhite HRs. Kind of silly.
image(kde_hr_sum_23)

#Mapping KDE home ranges onto study site at a 95% UD
sum23_kde_hr_ud <- plot(getverticeshr(kde_hr_sum_23, percent = 95))

#We can add either Raster_StudySite or SF_StudySite below.
plot(habitat.raster, alpha = 0.5, add = TRUE)
plot(Summer_2023_locs_KDE_SPDF, col = as.data.frame(Summer_2023_locs_KDE_SPDF)[,7], add = TRUE)

#Looking at a data frame of KDE HR sizes across various utilization distributions (e.g, 50%, 90%)
kde_levels_summer_2023 <- kernel.area(kde_hr_sum_23) 

#write.csv(covey_kde_hr_across_levels_2021_22,"/Users/jeffgrayum/Downloads/Clean_Covey_KDE_HR_2021_22.csv", row.names = TRUE)

kde_levels_summer_2023 %>%
  clean_names() %>%
  view()

kde_levels_summer_2023$ud <- rownames(kde_levels_summer_2023)

kde_levels_summer_2023_95 <- kde_levels_summer_2023 %>%
  dplyr::filter(ud == 95)

kde_levels_summer_2023_95_sel <- kde_levels_summer_2023_95 %>%
  dplyr::select(-ud)

kde_95_sum23 <- kde_levels_summer_2023_95_sel %>%
  pivot_longer(X4214:X20124937, names_to = "band_numb", values_to = "hr_size")

mean(kde_95_sum23$hr_size)

median(kde_95_sum23$hr_size)

min(kde_95_sum23$hr_size)

max(kde_95_sum23$hr_size)

kde_95_sum23 %>%
  ggplot(aes(hr_size)) +
  geom_histogram(bins = 10)


#Looking at data from both years
kde_95_sum22$Year <- 2022
kde_95_sum23$Year <- 2023

all_hr_95 <- rbind(kde_95_sum22, kde_95_sum23)

mean(all_hr_95$hr_size)

median(all_hr_95$hr_size)

min(all_hr_95$hr_size)

max(all_hr_95$hr_size)

all_hr_95 <- all_hr_95 %>%
  mutate(Year = as.factor(Year))

all_hr_95 %>%
  ggplot(aes(hr_size)) +
  geom_histogram(bins = 15, alpha = 0.8, fill = "midnight blue") +
  labs(x = "HR Size (95% UD)",
       y = "Count",
       title = "Distribution of home range sizes",
       subtitle = "Summer 2022 and Summer 2023")

all_hr_95 %>%
  ggplot(aes(hr_size, group = year, fill = Year)) +
  geom_histogram(bins = 15, alpha = 0.8) +
  labs(x = "HR Size (95% UD)",
       y = "Count",
       title = "Distribution of home range sizes",
       subtitle = "Summer 2022 and Summer 2023")

all_hr_95 %>%
  ggplot(aes(x = Year, y = hr_size, group = Year, fill = Year)) +
  geom_boxplot() +
  labs(y = "HR Size (ha)",
       title = "Distribution of HR sizes (ha)")

```

